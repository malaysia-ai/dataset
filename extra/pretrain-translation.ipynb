{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506b21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "690f598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = [\n",
    "    '/home/husein/ssd3/instructions/synthetic-codealpaca-v1-chatgpt4.jsonl',\n",
    "    '/home/husein/ssd3/instructions/synthetic-glaive_coder_raw_text.jsonl',\n",
    "    '/home/husein/ssd3/instructions/synthetic-oss_instruct-decontaminated.jsonl'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128be64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected_words = [\n",
    "    'kebutuhan',\n",
    "    'berbeda',\n",
    "    'bahwa',\n",
    "    'nomor',\n",
    "    'RMXX,XXX',\n",
    "    'kompleksitas',\n",
    "    'listrik',\n",
    "    'jawaban',\n",
    "    'teknis',\n",
    "    'berkualitas',\n",
    "    'mencoba',\n",
    "    'kampanye',\n",
    "    'komunitas',\n",
    "    'stabilitas',\n",
    "    'Stabilitas',\n",
    "    'metode',\n",
    "    'pria',\n",
    "    'butuh',\n",
    "    'jadwal',\n",
    "    'kasus',\n",
    "    'otomatis',\n",
    "    'populer',\n",
    "    'bisnis',\n",
    "    'probabilitas',\n",
    "    'rusak',\n",
    "    'kapasitas',\n",
    "    'rutinitas',\n",
    "    'pertama-tama',\n",
    "    'bagian',\n",
    "    'fungsionalitas',\n",
    "    'komoditas',\n",
    "    'diekspor',\n",
    "    ' akkan',\n",
    "    ' bisa',\n",
    "    'keren',\n",
    "    'konfigurkan',\n",
    "    'communique',\n",
    "    'berbeda',\n",
    "    'bagian-bagian',\n",
    "    'butuh',\n",
    "    'bahwa',\n",
    "    ' membaut'\n",
    "]\n",
    "\n",
    "indons = []\n",
    "def found_word(s, words):\n",
    "    global indons\n",
    "    for i in range(len(words)):\n",
    "        if words[i] in s:\n",
    "            indons.append(s)\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548a7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_code(s):\n",
    "    s = s.replace(' Kode ', ' Kod ')\n",
    "    s = s.replace(' kode ', ' kod ')\n",
    "    s = s.replace('Kode ', 'Kod ')\n",
    "    s = s.replace('kodenya', 'kodnya')\n",
    "    s = s.replace('Kodenya', 'Kodnya')\n",
    "    s = s.replace('opkode', 'opkod')\n",
    "    s = s.replace(' kode', ' kod')\n",
    "    s = s.replace('dikodekan', 'dikodkan')\n",
    "    s = s.replace('kodks', 'kodeks')\n",
    "    s = s.replace('kode ', 'kod ')\n",
    "    s = s.replace(' Kode', ' Kod')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20c9b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_news = glob('/home/husein/ssd3/google-translate-english-news/*.requested')\n",
    "len(english_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d95f9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_texts = glob('/home/husein/ssd3/google-translate-english-texts/*.requested')\n",
    "len(english_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458952e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malay_news = glob('/home/husein/ssd3/google-translate-malay-news/*.requested')\n",
    "len(malay_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b502b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hansard = glob('/home/husein/ssd3/google-translate-malaysia-hansard/*.requested')\n",
    "len(hansard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5de830f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = [english_news, english_texts]\n",
    "mss = [malay_news, hansard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e692c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject(k):\n",
    "    if 'return JSON' in k:\n",
    "        return\n",
    "    if 'AI language model' in k:\n",
    "        return\n",
    "    if 'model bahasa AI' in k:\n",
    "        return\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfec1b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = [\n",
    "    'zh-CN', 'ta', 'jw', 'id'\n",
    "]\n",
    "\n",
    "sublanguages = []\n",
    "for f in folders:\n",
    "    sublanguages.extend(glob(f'/home/husein/ssd3/google-translate-ms-{f}/ms*.requested'))\n",
    "    \n",
    "len(sublanguages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4be746f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'w') as fopen_jsonl:\n",
    "\n",
    "    with open('/home/husein/ssd3/llama2/postfilter.jsonl') as fopen:\n",
    "        for l in fopen:\n",
    "            l = json.loads(l)\n",
    "\n",
    "            en = l['instruction']\n",
    "            ms = l['instruction_ms']\n",
    "\n",
    "            if not en or not ms:\n",
    "                continue\n",
    "\n",
    "            if not reject(en) or not reject(ms):\n",
    "                continue\n",
    "\n",
    "            ms = replace_code(ms)\n",
    "\n",
    "            if found_word(ms, rejected_words):\n",
    "                continue\n",
    "\n",
    "            if len(en) and len(ms):\n",
    "                \n",
    "                s = f'Malay to English: {ms} {en}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()\n",
    "                \n",
    "                s = f'English to Malay: {en} {ms}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016dde3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'a') as fopen_jsonl:\n",
    "    \n",
    "    for f in selected:\n",
    "        with open(f) as fopen:\n",
    "            for l in fopen:\n",
    "                l = json.loads(l)\n",
    "                en = l['instruction_en']\n",
    "                ms = l['instruction_ms']\n",
    "\n",
    "                if not en or not ms:\n",
    "                    continue\n",
    "\n",
    "                if not reject(en) or not reject(ms):\n",
    "                    continue\n",
    "                \n",
    "                ms = replace_code(ms)\n",
    "        \n",
    "                if found_word(ms, rejected_words):\n",
    "                    continue\n",
    "                \n",
    "                if len(en) and len(ms):\n",
    "                    s = f'Malay to English: {ms} {en}'\n",
    "                    \n",
    "                    fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                    fopen_jsonl.flush()\n",
    "\n",
    "                    s = f'English to Malay: {en} {ms}'\n",
    "\n",
    "                    fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                    fopen_jsonl.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "076119e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'a') as fopen_jsonl:\n",
    "    with open('/home/husein/ssd3/chatgpt4-noisy-translation-twitter-dialect/negeri-kelantan.jsonl') as fopen:\n",
    "        for l in fopen:\n",
    "            l = json.loads(l)\n",
    "            left = l['r']['translation']\n",
    "            right = l['original']['ms']\n",
    "            \n",
    "            if found_word(right, rejected_words):\n",
    "                continue\n",
    "            \n",
    "            if len(left) and len(left.split()) < 2000 and len(right) and len(right.split()) < 2000:\n",
    "\n",
    "                s = f'Kelantanese to Standard Malay: {left} {right}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()\n",
    "            \n",
    "    with open('/home/husein/ssd3/chatgpt4-noisy-translation-twitter-dialect/negeri-utara.jsonl') as fopen:\n",
    "        for l in fopen:\n",
    "            l = json.loads(l)\n",
    "            left = l['r']['translation']\n",
    "            right = l['original']['ms']\n",
    "            \n",
    "            if found_word(right, rejected_words):\n",
    "                continue\n",
    "            \n",
    "            if len(left) and len(left.split()) < 2000 and len(right) and len(right.split()) < 2000:\n",
    "\n",
    "                s = f'Northern language to Standard Malay: {left} {right}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7c2a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "manglish = hf_hub_download(\n",
    "    repo_id=\"mesolitica/chatgpt-noisy-translation-manglish\", \n",
    "    filename=\"process-manglish.jsonl\", repo_type = 'dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "564b5a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1036411it [00:18, 55162.55it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('train.json', 'a') as fopen_jsonl:\n",
    "    with open(manglish) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            l = json.loads(l)\n",
    "            left = l['left']\n",
    "            en = l['en']\n",
    "            ms = l['ms']\n",
    "            \n",
    "            if not reject(en) or not reject(ms):\n",
    "                continue\n",
    "\n",
    "            if len(left) and len(left.split()) < 2000 and len(en) and len(en.split()) < 2000:\n",
    "                \n",
    "                s = f'Manglish to Standard English: {left} {en}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()\n",
    "                \n",
    "                s = f'Standard English to Manglish: {en} {left}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()\n",
    "                \n",
    "            if found_word(ms, rejected_words):\n",
    "                continue\n",
    "                \n",
    "            if len(left) and len(left.split()) < 2000 and len(ms) and len(ms.split()) < 2000:\n",
    "                \n",
    "                s = f'Manglish to Standard Malay: {left} {ms}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()\n",
    "                \n",
    "                s = f'Standard Malay to Manglish: {ms} {left}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fe84a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "bjn = hf_hub_download(\n",
    "    repo_id=\"mesolitica/chatgpt-noisy-translation-banjarese\", \n",
    "    filename=\"process-bjn-Latn.jsonl\", repo_type = 'dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76b804ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "958405it [00:14, 64478.33it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('train.json', 'a') as fopen_jsonl:\n",
    "    with open(bjn) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            l = json.loads(l)\n",
    "            left = l['left']\n",
    "            chatgpt = l['chatgpt']\n",
    "            google = l['google']\n",
    "            \n",
    "            if not reject(chatgpt) or not reject(google):\n",
    "                continue\n",
    "                \n",
    "            if found_word(google, rejected_words):\n",
    "                continue\n",
    "            \n",
    "            if len(left) and len(left.split()) < 2000 and len(google) and len(google.split()) < 2000:\n",
    "                \n",
    "                s = f'Banjarese to Malay: {left} {google}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "979b9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = hf_hub_download(\n",
    "    repo_id=\"mesolitica/chatgpt-noisy-translation-facebook\", \n",
    "    filename=\"processed-facebook.jsonl\", repo_type = 'dataset'\n",
    ")\n",
    "twitter = hf_hub_download(\n",
    "    repo_id=\"mesolitica/chatgpt-noisy-translation-twitter\", \n",
    "    filename=\"processed-twitter.jsonl\", repo_type = 'dataset'\n",
    ")\n",
    "iium = hf_hub_download(\n",
    "    repo_id=\"mesolitica/chatgpt-noisy-translation-iium-confession\", \n",
    "    filename=\"processed-iium-confession.jsonl\", repo_type = 'dataset'\n",
    ")\n",
    "bcari = hf_hub_download(\n",
    "    repo_id=\"mesolitica/chatgpt-noisy-translation-b.cari.com.my\", \n",
    "    filename=\"processed-b.cari.com.my.jsonl\", repo_type = 'dataset'\n",
    ")\n",
    "ccari = hf_hub_download(\n",
    "    repo_id=\"mesolitica/chatgpt-noisy-translation-c.cari.com.my\", \n",
    "    filename=\"processed-c.cari.com.my.jsonl\", repo_type = 'dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b5c12cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "691013it [00:09, 75032.24it/s]\n",
      "141851it [00:01, 85171.61it/s]\n",
      "333758it [00:08, 40754.50it/s]\n",
      "631727it [00:11, 53557.44it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('train.json', 'a') as fopen_jsonl:\n",
    "    for f in [\n",
    "        twitter, \n",
    "        fb, \n",
    "        iium,\n",
    "        bcari,\n",
    "    ]:\n",
    "        with open(f) as fopen:\n",
    "            for l in tqdm(fopen):\n",
    "                try:\n",
    "                    l = json.loads(l)\n",
    "                    left = l['left']\n",
    "                    en = l['en']\n",
    "                    ms = l['ms']\n",
    "                    \n",
    "                    if not reject(en) or not reject(ms):\n",
    "                        continue\n",
    "\n",
    "                    if len(left) and len(left.split()) < 2000 and len(en) and len(en.split()) < 2000:\n",
    "                        s = f'social media Malay to standard English: {left} {en}'\n",
    "                    \n",
    "                        fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                        fopen_jsonl.flush()\n",
    "                        \n",
    "                    if found_word(ms, rejected_words):\n",
    "                        continue\n",
    "\n",
    "                    if len(left) and len(left.split()) < 2000 and len(ms) and len(ms.split()) < 2000:\n",
    "                        s = f'social media Malay to standard Malay: {left} {ms}'\n",
    "                    \n",
    "                        fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                        fopen_jsonl.flush()\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5554515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"social media Malay to standard Malay: Hadoi. Tapi okay la ni at least cina vs cina. Kalau cina vs melayu or cina vs india kang kata racist pulak. Hadoi. Tapi tak apa la, setidaknya ini pertembungan antara orang Cina dengan orang Cina. Kalau pertembungan antara orang Cina dengan orang Melayu atau orang Cina dengan orang India, orang akan kata itu rasis.\"\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 1 train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcf046b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "740431it [00:26, 28466.65it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('train.json', 'a') as fopen_jsonl:\n",
    "    for f in [\n",
    "        ccari,\n",
    "    ]:\n",
    "        with open(f) as fopen:\n",
    "            for l in tqdm(fopen):\n",
    "                try:\n",
    "                    l = json.loads(l)\n",
    "                    left = l['left']\n",
    "                    en = l['en']\n",
    "                    ms = l['ms']\n",
    "                    \n",
    "                    if not reject(en) or not reject(ms):\n",
    "                        continue\n",
    "\n",
    "                    if len(left) and len(left.split()) < 2000 and len(en) and len(en.split()) < 2000:\n",
    "                        s = f'social media Mandarin to standard English: {left} {en}'\n",
    "                    \n",
    "                        fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                        fopen_jsonl.flush()\n",
    "                        \n",
    "                    if found_word(ms, rejected_words):\n",
    "                        continue\n",
    "\n",
    "                    if len(left) and len(left.split()) < 2000 and len(ms) and len(ms.split()) < 2000:\n",
    "                        s = f'social media Mandarin to standard Malay: {left} {ms}'\n",
    "                    \n",
    "                        fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                        fopen_jsonl.flush()\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "852fe8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"social media Mandarin to standard Malay: \\u56e0\\u4e3a\\u6211\\u6709\\u4e2afree room\\u6240\\u4ee5\\u6253\\u7b97\\u53ea\\u662f\\u53bb\\u4e2ashort trip\\u90a3\\u6837\\u800c\\u5df2  \\u8001\\u516c\\u53c8\\u4e0d\\u80fd\\u62ff\\u592a\\u591a\\u5047\\u671f\\u3002  \\u4e00\\u5929\\u7684\\u8bdd\\u6211\\u89c9\\u5f97\\u53ef\\u4ee5\\u53bbcable car+sky bridge (sky bridge\\u4f1a\\u722c\\u5230\\u8981\\u6b7b\\uff0c\\u4e0d\\u597d\\u8fd0\\u52a8\\u7684\\u8bdd\\u5c31\\u4e0d\\u8981\\u53bb\\u4e86\\uff09  \\u4e0d\\u8fc7\\u4f60\\u8981\\u67e5\\u770b\\u4f60\\u53bb\\u7684\\u65f6\\u5019\\u4f1a\\u4e0d\\u4f1a\\u649e\\u5230\\u4ed6maintenance\\u7684\\u65f6\\u95f4\\u3002  \\u6211\\u4e00\\u76f4\\u90fd\\u5728pantai cenang\\u90a3\\uff0ckuah town\\u6211\\u5c31\\u6ca1\\u53bb\\u8fc7\\uff0c\\u90a3\\u91cc\\u5e94\\u8be5\\u4e5f\\u662f\\u6709\\u4e1c\\u897f\\u8d70\\u7684\\uff08\\u8001\\u9e70\\u5e7f\\u573a\\uff09  \\u5bf9\\u4e86\\uff0c\\u4f60\\u7684\\u9152\\u5e97\\u5728\\u54ea\\u7684\\uff1f\\u5c31\\u770b\\u770b\\u9644\\u8fd1\\u90a3\\u6709\\u4ec0\\u4e48\\u505a\\u5c31\\u597d\\u4e86~  \\u6211\\u548c\\u7537\\u53cb\\u7684\\u65c5\\u884c\\u4e00\\u76f4\\u90fd\\u662f\\u5728hea\\u7684\\uff0c\\u4ed6\\u4e0d\\u7231\\u8dd1\\uff0c\\u52a0\\u4e0alangkawi\\u6211\\u4eec\\u4e00\\u76f4\\u90fd\\u53bb\\uff0c\\u6240\\u4ee5\\u4e5f\\u89c9\\u5f97\\u6ca1\\u4ec0\\u4e48\\u4e1c\\u897f\\u503c\\u5f97\\u53bb\\u770b\\u54af  \\u4e0d\\u7136\\u4e5f\\u6709wildlife park\\u7684\\uff0c\\u53ef\\u662f\\uff0c\\u6211\\u4eec\\u90fd\\u4e0d\\u611f\\u5174\\u8da3  \\u542c\\u8bf4tanjung rhu\\u6c99\\u6ee9\\u5f88\\u6f02\\u4eae\\uff0c\\u4e0d\\u8fc7\\u6211\\u4eec\\u4e5f\\u662f\\u6ca1\\u53bb\\u5230~ Kerana saya mempunyai bilik percuma, saya merancang untuk pergi hanya untuk perjalanan singkat sahaja. Suami saya tidak boleh mengambil cuti terlalu lama. Jika hanya untuk sehari, saya rasa kita boleh pergi ke kereta kabel dan jambatan langit (jambatan langit memerlukan sedikit pendakian, jadi jika anda tidak mahir dalam aktiviti fizikal, lebih baik jangan pergi). Walau bagaimanapun, anda perlu memeriksa sama ada ia bertepatan dengan jadual penyelenggaraan mereka ketika anda pergi. Saya sentiasa berada di Pantai Cenang, saya belum pernah pergi ke Kuah Town, tetapi pastinya ada aktiviti yang boleh dilakukan di sana juga (Eagle Square). By the way, hotel anda terletak di mana? Cari sahaja apa yang boleh dilakukan di sekitar kawasan tersebut. Kawan lelaki saya dan saya sentiasa mengutamakan perjalanan yang santai, dia tidak suka tergesa-gesa, dan kerana kami telah pergi ke Langkawi banyak kali, kami rasa tidak banyak tempat yang berbaloi untuk dikunjungi lagi. Jika tidak, terdapat taman hidupan liar, tetapi kami tidak berminat. Saya mendengar Pantai Tanjung Rhu sangat cantik, tetapi kami juga tidak berkesempatan untuk pergi ke sana.\"\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 1 train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cebb23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'a') as fopen_jsonl:\n",
    "    with open('/home/husein/ssd3/google-translate-malay-news/translation-manglish.jsonl') as fopen:\n",
    "        for l in fopen:\n",
    "            try:\n",
    "                l = json.loads(l)\n",
    "                en = l['translation'].get('standard_en', '')\n",
    "                ms = l['translation'].get('standard_ms', '')\n",
    "                left = l['text']\n",
    "\n",
    "                if len(en) and len(en.split()) < 2000 and len(left) and len(left.split()) < 2000:\n",
    "                    s = f'Manglish to standard English: {left} {en}'\n",
    "                    \n",
    "                    fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                    fopen_jsonl.flush()\n",
    "\n",
    "                if found_word(ms, rejected_words):\n",
    "                    continue\n",
    "\n",
    "                if len(ms) and len(ms.split()) < 2000 and len(left) and len(left.split()) < 2000:\n",
    "                    s = f'Manglish to standard Malay: {left} {ms}'\n",
    "                    \n",
    "                    fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                    fopen_jsonl.flush()\n",
    "                \n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2fed128",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'a') as fopen_jsonl:\n",
    "    with open('/home/husein/ssd3/google-translate-malay-news/translation-c.cari.com.my.jsonl') as fopen:\n",
    "        for l in fopen:\n",
    "            try:\n",
    "                l = json.loads(l)\n",
    "                en = l['translation'].get('standard_en', '')\n",
    "                ms = l['translation'].get('standard_ms', '')\n",
    "                left = l['text']\n",
    "\n",
    "                if len(en) and len(en.split()) < 2000 and len(left) and len(left.split()) < 2000:\n",
    "                    s = f'social media Mandarin to standard English: {left} {en}'\n",
    "                    \n",
    "                    fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                    fopen_jsonl.flush()\n",
    "\n",
    "\n",
    "                if found_word(ms, rejected_words):\n",
    "                    continue\n",
    "\n",
    "                if len(ms) and len(ms.split()) < 2000 and len(left) and len(left.split()) < 2000:\n",
    "                    s = f'social media Mandarin to standard Malay: {left} {ms}'\n",
    "                    \n",
    "                    fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                    fopen_jsonl.flush()\n",
    "                    \n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef825e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'a') as fopen_jsonl:\n",
    "    with open('/home/husein/ssd3/google-translate-ms-zh-CN/wiki-jawi-ms-en.jsonl') as fopen:\n",
    "        for l in fopen:\n",
    "            l = json.loads(l)\n",
    "            en = l['en']\n",
    "            ms = l['malay']\n",
    "            left = l['jawi']\n",
    "\n",
    "            if len(en) and len(en.split()) < 2000 and len(left) and len(left.split()) < 2000:\n",
    "                \n",
    "                s = f'Standard English to Jawi: {en} {left}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()\n",
    "                \n",
    "            if len(ms) and len(ms.split()) < 2000 and len(left) and len(left.split()) < 2000:\n",
    "                \n",
    "                s = f'Standard Malay to Jawi: {ms} {left}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0759a26d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Standard Malay to Jawi: Wilayah Plzen (bahasa Czech Plzensky kraj ) merupakan sebuah daerah di Czech yang memiliki keluasan wilayah 7,561 km2 dan populasi 551,528 orang pada (2006). Ibu kotanya ialah Plzen. Pembahagian pentadbiran. Komun. Sejak 1 Januari 2003, wilayah ini terbahagi kepada 15 Kawasan perbandaran \\u0648\\u0644\\u0627\\u064a\\u0647 \\u06a4\\u0644\\u0632\\u064a\\u0646 \\ufd3f\\u0628\\u0647\\u0627\\u0633 \\u0686\\u0632\\u064a\\u0686\\u0647 \\u06a4\\u0644\\u0632\\u064a\\u0646\\u0633\\u0643\\u0627\\u064a \\u0643\\u0631\\u0627\\u062c \\ufd3e \\u0645\\u0631\\u0648\\u06a4\\u0627\\u0643\\u0646 \\u0633\\u0628\\u0648\\u0627\\u0647 \\u062f\\u0627\\u064a\\u0631\\u0647 \\u062f \\u0686\\u0632\\u064a\\u0686\\u0647 \\u064a\\u06a0 \\u0645\\u0645\\u064a\\u0644\\u064a\\u0642\\u064a \\u0643\\u0644\\u0648\\u0627\\u0633\\u0646 \\u0648\\u0644\\u0627\\u064a\\u0647 \\u0667\\u060c\\u0665\\u0666\\u0661 \\u0642\\u0645\\u0662 \\u062f\\u0627\\u0646 \\u06a4\\u0648\\u06a4\\u0648\\u0644\\u0633\\u064a \\u0665\\u0665\\u0661\\u060c\\u0665\\u0662\\u0668 \\u0627\\u0648\\u0631\\u06a0 \\u06a4\\u062f \\ufd3f\\u0662\\u0660\\u0660\\u0666\\ufd3e. \\u0627\\u064a\\u0628\\u0648 \\u0643\\u0648\\u062a\\u0646\\u064a\\u0627 \\u0627\\u064a\\u0627\\u0644\\u0647 \\u06a4\\u0644\\u0632\\u064a\\u0646. \\u06a4\\u0645\\u0628\\u0647\\u0627\\u06ac\\u064a\\u0646 \\u06a4\\u064a\\u0646\\u062a\\u0627\\u062f\\u0628\\u064a\\u0631\\u0627\\u0646. \\u0643\\u0648\\u0645\\u0648\\u0646. \\u0633\\u062c\\u0642 \\u0661 \\u062c\\u0627\\u0646\\u0648\\u0627\\u0631\\u064a \\u0662\\u0660\\u0660\\u0663\\u060c \\u0648\\u0644\\u0627\\u064a\\u0647 \\u0627\\u064a\\u0646 \\u062a\\u0631\\u0628\\u0647\\u0627\\u06ac\\u064a \\u0643\\u06a4\\u062f \\u0661\\u0665 \\u0643\\u0627\\u0648\\u0633\\u0646 \\u06a4\\u0631\\u0628\\u0646\\u062f\\u0631\\u0646\"\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 1 train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b602d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cfb60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'a') as fopen_jsonl:\n",
    "    with open('/home/husein/ssd3/google-translate-malay-news/synthetic-word-switching-translation.jsonl') as fopen:\n",
    "        for l in fopen:\n",
    "            l = json.loads(l)\n",
    "            ms = l['ms']\n",
    "            en = l['en']\n",
    "            if not len(l['augmentation']):\n",
    "                continue\n",
    "            augmentation = random.sample(l['augmentation'], min(len(l['augmentation']), 2))\n",
    "            for a in augmentation:\n",
    "                \n",
    "                s = f'to English: {a} {en}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()\n",
    "                \n",
    "                s = f'to Malay: {a} {ms}'\n",
    "                    \n",
    "                fopen_jsonl.write(f'{json.dumps(s)}\\n')\n",
    "                fopen_jsonl.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98aef5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"to Malay: Tok expect \\u06a4\\u0648\\u0644\\u0627 filem pertama kaih mendapat \\u0644\\u0627\\u06a4\\u0646 \\u06a4\\u0646\\u0686\\u0644\\u0648\\u0646\\u0646 dalam FFM28. Tidak sangka pula filem pertama saya mendapat lapan pencalonan dalam FFM28.\"\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 1 train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a1a83ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 husein husein 11G Ogos  5 16:25 train.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lha train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20349e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ccb3c1a817f4a7e8582d0be722c3815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.json:   0%|          | 0.00/11.4G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"train.json\",\n",
    "    path_in_repo=\"pretrain-translation.jsonl\",\n",
    "    repo_id=\"malaysia-ai/pretrain-text-dataset\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1ae00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
