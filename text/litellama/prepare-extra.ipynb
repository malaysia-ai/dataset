{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc24e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/syafie-nzm/crawl-jurnaldbp/resolve/main/jurnaldbp.jsonl\n",
    "# !wget https://huggingface.co/datasets/mesolitica/crawl-my-website/resolve/main/mjpharm.org.jsonl\n",
    "# !wget https://huggingface.co/datasets/mesolitica/crawl-my-website/resolve/main/myjgeosc.com.jsonl\n",
    "# !wget https://huggingface.co/datasets/mesolitica/crawl-my-website/resolve/main/myjsustainagri.com.jsonl\n",
    "# !wget https://huggingface.co/datasets/mesolitica/crawl-my-website/resolve/main/akademisains.gov.my.jsonl\n",
    "# !wget https://huggingface.co/datasets/mesolitica/crawl-my-website/resolve/main/crossref-pdf.jsonl\n",
    "# !wget https://huggingface.co/datasets/mesolitica/crawl-my-website/resolve/main/Kamus_Dewan_Bahasa_Edisi_Keempat_pdf.pdf\n",
    "# !wget https://huggingface.co/datasets/mesolitica/crawl-my-website/resolve/main/melayu-pdf.jsonl\n",
    "# !wget https://huggingface.co/datasets/mesolitica/crawl-my-website/resolve/main/majcafe.com.jsonl\n",
    "# !wget https://huggingface.co/datasets/mesolitica/crawl-my-website/resolve/main/myjms.mohe.gov.my.jsonl\n",
    "# !wget https://huggingface.co/datasets/mesolitica/crawl-my-website/resolve/main/newera.edu.my.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c99b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from streaming import MDSWriter\n",
    "from tqdm import tqdm\n",
    "import msgspec\n",
    "import json\n",
    "import re\n",
    "\n",
    "http_errors = [\n",
    "        \"400 Bad Request\", \"401 Unauthorized\", \"402 Payment Required\", \"403 Forbidden\", \"404 Not Found\",\n",
    "        \"405 Method Not Allowed\", \"406 Not Acceptable\", \"407 Proxy Authentication Required\", \"408 Request Timeout\",\n",
    "        \"409 Conflict\", \"410 Gone\", \"411 Length Required\", \"412 Precondition Failed\", \"413 Payload Too Large\",\n",
    "        \"414 URI Too Long\", \"415 Unsupported Media Type\", \"416 Range Not Satisfiable\", \"417 Expectation Failed\",\n",
    "        \"418 I'm a teapot\", \"421 Misdirected Request\", \"422 Unprocessable Entity\", \"423 Locked\", \"424 Failed Dependency\",\n",
    "        \"425 Too Early\", \"426 Upgrade Required\", \"428 Precondition Required\", \"429 Too Many Requests\",\n",
    "        \"431 Request Header Fields Too Large\", \"451 Unavailable For Legal Reasons\", \"500 Internal Server Error\",\n",
    "        \"501 Not Implemented\", \"502 Bad Gateway\", \"503 Service Unavailable\", \"504 Gateway Timeout\",\n",
    "        \"505 HTTP Version Not Supported\", \"506 Variant Also Negotiates\", \"507 Insufficient Storage\",\n",
    "        \"508 Loop Detected\", \"510 Not Extended\", \"511 Network Authentication Required\"\n",
    "    ]\n",
    "\n",
    "rejected = [\n",
    "    'Internal Server Error',\n",
    "    '__NOEDITSECTION__',\n",
    "    'enter your username and password',\n",
    "    'forgotten your password',\n",
    "    'cookies enabled',\n",
    "    'enable JavaScript in your browser.',\n",
    "    'The page cannot be displayed',\n",
    "    'site or edit the error_page',\n",
    "    'Request unsuccessful',\n",
    "]\n",
    "\n",
    "rejected.extend(http_errors)\n",
    "\n",
    "def replace_multiple(input_string, pattern =r\"\\s{6,}\", replace = '   '):\n",
    "    return re.sub(pattern, replace, input_string)\n",
    "\n",
    "def replace(string):\n",
    "    string = replace_multiple(string.replace('â€¦', '.'))\n",
    "    string = replace_multiple(string, pattern = r\"\\.{6,}\", replace = '...')\n",
    "    return string\n",
    "\n",
    "def reject(string):\n",
    "    if any([r in string for r in rejected]):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c2023b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "\n",
    "class UInt16(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint16)\n",
    "\n",
    "_encodings['uint16'] = UInt16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f868397",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'jurnaldbp.jsonl',\n",
    "    'mjpharm.org.jsonl',\n",
    "    'myjgeosc.com.jsonl',\n",
    "    'myjsustainagri.com.jsonl',\n",
    "    'akademisains.gov.my.jsonl',\n",
    "    'crossref-pdf.jsonl',\n",
    "    'Kamus_Dewan_Bahasa_Edisi_Keempat_pdf.pdf',\n",
    "    'melayu-pdf.jsonl',\n",
    "    'majcafe.com.jsonl',\n",
    "    'myjms.mohe.gov.my.jsonl',\n",
    "    'newera.edu.my.jsonl'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e7a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    'input_ids': 'uint16',\n",
    "}\n",
    "compression = 'zstd'\n",
    "hashes = 'sha1', 'xxh64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bd52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(text, size = 500):\n",
    "    splitted = text.split()\n",
    "    return [' '.join(splitted[i: i + size]) for i in range(0, len(splitted), size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4cf6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "930it [00:02, 380.24it/s]\n",
      "90it [00:00, 177.15it/s]\n",
      "100it [00:00, 440.49it/s]\n",
      "119it [00:00, 453.21it/s]\n",
      "811it [00:01, 414.01it/s]\n",
      "62149it [04:10, 248.15it/s]\n",
      "77181it [00:00, 254320.05it/s]IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "155282it [00:00, 240440.67it/s]IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "230240it [00:00, 260177.61it/s]\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "1506it [00:04, 353.44it/s]\n",
      "490it [00:01, 472.75it/s]\n",
      "16419it [00:27, 589.29it/s]\n",
      "87it [00:00, 762.47it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('extra.jsonl', 'w') as fopen_l:\n",
    "    for f in files:\n",
    "        with open(f, encoding = \"ISO-8859-1\") as fopen:\n",
    "            for l in tqdm(fopen):\n",
    "                try:\n",
    "                    l = msgspec.json.decode(l)\n",
    "                    if reject(l):\n",
    "                        continue\n",
    "\n",
    "                    l = replace(l.strip())\n",
    "                    if len(l) < 3:\n",
    "                        continue\n",
    "                        \n",
    "                    data = '<s>' + l + '</s>'\n",
    "                    partitioned = partition(data)\n",
    "                    for p in partitioned:\n",
    "                        data = {\n",
    "                            'text': p,\n",
    "                        }\n",
    "                        fopen_l.write(f'{json.dumps(data)}\\n')\n",
    "                        fopen_l.flush()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ba46e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_by = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf91a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir partitions-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "516d9cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1286178it [00:07, 166775.26it/s]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "count = 0\n",
    "a = open(f'partitions-extra/combined-lm-{index}.jsonl', 'w')\n",
    "\n",
    "with open('extra.jsonl') as fopen:\n",
    "    for l in tqdm(fopen):\n",
    "        a.write(l)\n",
    "        a.flush()\n",
    "        count += 1\n",
    "        if count >= split_by:\n",
    "            a.close()\n",
    "            index += 1\n",
    "            count = 0\n",
    "            a = open(f'partitions-extra/combined-lm-{index}.jsonl', 'w')\n",
    "            \n",
    "a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2c5a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 4096\n",
    "\n",
    "def read_dataset(train_file, block_size = block_size):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'ahxt/LiteLlama-460M-1T',\n",
    "    )\n",
    "    tokenizer.add_bos_token = False\n",
    "    tokenizer.add_eos_token = False\n",
    "    tokenizer.model_max_length = 32768\n",
    "    special_tokens_dict = {\"eos_token\": \"</s>\", \"bos_token\": '<s>'}\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    text_column_name = 'text'\n",
    "    temp = []\n",
    "    with open(train_file) as fopen:\n",
    "        for l in fopen:\n",
    "            l = msgspec.json.decode(l)\n",
    "            tokenized = tokenizer(l['text'])['input_ids']\n",
    "            temp.extend(tokenized)\n",
    "            while len(temp) >= block_size:\n",
    "                block = temp[:block_size]\n",
    "                temp = temp[block_size:]\n",
    "                if len(block) == block_size:\n",
    "                    yield np.array(block).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37217f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['partitions-extra/combined-lm-0.jsonl',\n",
       " 'partitions-extra/combined-lm-1.jsonl',\n",
       " 'partitions-extra/combined-lm-2.jsonl',\n",
       " 'partitions-extra/combined-lm-3.jsonl',\n",
       " 'partitions-extra/combined-lm-4.jsonl',\n",
       " 'partitions-extra/combined-lm-5.jsonl',\n",
       " 'partitions-extra/combined-lm-6.jsonl',\n",
       " 'partitions-extra/combined-lm-7.jsonl',\n",
       " 'partitions-extra/combined-lm-8.jsonl',\n",
       " 'partitions-extra/combined-lm-9.jsonl',\n",
       " 'partitions-extra/combined-lm-10.jsonl',\n",
       " 'partitions-extra/combined-lm-11.jsonl',\n",
       " 'partitions-extra/combined-lm-12.jsonl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = sorted(glob('partitions-extra/combined-lm-*.jsonl'), key = lambda x: int(x.split('-')[-1].replace('.jsonl', '')))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd49a09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1352 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([50258, 25270,   509, ...,  3129,   272,  3112], dtype=uint16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(read_dataset(files[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a0205ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf tokenized_extra\n",
    "!mkdir tokenized_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "64d7e662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(files):\n",
    "    files, index = files\n",
    "    out_root = f'tokenized_extra/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=compression, hashes=hashes) as out:\n",
    "        for f in files:\n",
    "            for block in tqdm(read_dataset(train_file = f)):\n",
    "                sample = {\n",
    "                    'input_ids': block\n",
    "                }\n",
    "                out.write(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f4d6b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://gist.githubusercontent.com/huseinzol05/98974ae8c6c7a65d4bc0af9f5003786a/raw/5aa5257608b61e8fcc828e99fbd070d5ca7358e3/mp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e24711db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "0it [00:00, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "18096it [02:17, 131.63it/s]\n",
      "20019it [02:40, 124.96it/s]\n",
      "20029it [02:41, 124.09it/s]\n",
      "20072it [02:41, 124.14it/s]\n",
      "20087it [02:42, 123.47it/s]\n",
      "20139it [02:44, 122.61it/s]\n",
      "20114it [02:44, 122.31it/s]\n",
      "20299it [02:44, 123.17it/s]\n",
      "19945it [02:46, 120.03it/s]\n",
      "20004it [02:49, 118.15it/s]\n",
      "20021it [02:49, 118.22it/s]\n",
      "24242it [02:59, 135.31it/s]\n",
      "21222it [03:01, 117.16it/s]\n"
     ]
    }
   ],
   "source": [
    "import mp\n",
    "mp.multiprocessing(files, loop, cores = min(len(files), 20), returned = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47249c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "'ahxt/LiteLlama-460M-1T',\n",
    ")\n",
    "tokenizer.add_bos_token = False\n",
    "tokenizer.add_eos_token = False\n",
    "tokenizer.model_max_length = 32768\n",
    "special_tokens_dict = {\"eos_token\": \"</s>\", \"bos_token\": '<s>'}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97a3e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/streaming/base/dataset.py:397: UserWarning: Because `predownload` was not specified, it will default to 8*batch_size if batch_size is not None, otherwise 64. Prior to Streaming v0.7.0, `predownload` defaulted to max(batch_size, 256 * batch_size // num_canonical_nodes).\n",
      "  warnings.warn(f'Because `predownload` was not specified, it will default to ' +\n"
     ]
    }
   ],
   "source": [
    "from streaming import StreamingDataset\n",
    "\n",
    "dataset = StreamingDataset(local = 'tokenized_extra/tokenized-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551c675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
