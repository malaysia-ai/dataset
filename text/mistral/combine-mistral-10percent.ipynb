{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43a27aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/mesolitica/translated-unnatural_code_instructions_20M/resolve/main/unnatural-instructions.jsonl.requested\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-python-evol-instruct-51k/resolve/main/python_evol_instruct_51k.jsonl.requested\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-code-instructions-122k/resolve/main/code_instructions_120k.jsonl.requested\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-glaive_coder_raw_text/resolve/main/glaive_coder_raw_text.jsonl.requested\n",
    "# !wget https://huggingface.co/datasets/mesolitica/chatgpt-gpt4all/resolve/main/translated-gpt4all-code.json\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-WizardLM_evol_instruct_V2_196k/resolve/main/WizardLM_evol_instruct_V2_143k.translated.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79243f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/mesolitica/translated-math_qa/resolve/main/math-qa.jsonl.translated\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-mini-math23k-v1/resolve/main/mini-math23k.jsonl.requested\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-MathInstruct/resolve/main/math-instruct.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5caef46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e740b67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkout = \"\"\"\n",
    "NLLB.jsonl\n",
    "2.44 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "academia-edu.jsonl\n",
    "260 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "agendadaily.jsonl\n",
    "50.1 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ajar.jsonl\n",
    "2.58 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "akuislam.com.jsonl\n",
    "1.01 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "aliffchannel.jsonl\n",
    "6.19 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "amanz.jsonl\n",
    "43.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "article.poliklinikazzaara.com.my.jsonl\n",
    "282 kB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "artikel.jsonl\n",
    "252 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "asklegal.jsonl\n",
    "8.36 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "autobuzz.jsonl\n",
    "23.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "azhafizah.com.jsonl\n",
    "13.1 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "b.cari.com.my.jsonl\n",
    "1.45 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "beautifulnara.jsonl\n",
    "7.07 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "bidadari.jsonl\n",
    "14.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "bjak.my.jsonl\n",
    "9.8 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "blog.fincrew.my.jsonl\n",
    "1.48 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "blog.limkitsiang.com.jsonl\n",
    "94.7 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "blog.malaysia-asia.jsonl\n",
    "1.24 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "blog.pandai.com.jsonl\n",
    "916 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "blogmalaysia-com.jsonl\n",
    "8.2 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "blogtipskerjaya.net.jsonl\n",
    "4.14 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "bullishbursa.blogspot.com.jsonl\n",
    "137 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "bumiinvest20.home.blog.jsonl\n",
    "298 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "buro247.jsonl\n",
    "46 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "carlist-my.jsonl\n",
    "36 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "carsifu-my.jsonl\n",
    "61 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "carsome-my.jsonl\n",
    "7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "columbiaasia.com.jsonl\n",
    "2.13 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "common-crawl.jsonl\n",
    "2.35 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "data.gov.my.jsonl\n",
    "1.03 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "denaihati.jsonl\n",
    "16.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "dewanbahasa-jdbp.jsonl\n",
    "17 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "dialect.jsonl\n",
    "16.4 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "dictionary.jsonl\n",
    "28 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "discoverkl.jsonl\n",
    "6.11 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "diva-my.jsonl\n",
    "14.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "doctoroncall.jsonl\n",
    "87 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "dotproperty.com.my.jsonl\n",
    "2.65 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "dsf-my.jsonl\n",
    "99.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "e-khutbah.jsonl\n",
    "27.7 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ecentral.my.jsonl\n",
    "8.75 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "edu.my.jsonl\n",
    "531 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "ekonomirakyat.com.jsonl\n",
    "886 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "eprints.jsonl\n",
    "5.2 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ering.jsonl\n",
    "16.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "fiksyenshasha.jsonl\n",
    "78.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "fintechnews.my.jsonl\n",
    "3.73 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "fuh.my.jsonl\n",
    "4.08 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "gamerbraves.com.jsonl\n",
    "44.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "gamersantai.com.jsonl\n",
    "44.3 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "gamersonduty.com.jsonl\n",
    "1.97 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "garis-panduan.jsonl\n",
    "237 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "gempak.com.jsonl\n",
    "50.3 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "goodymy.com.jsonl\n",
    "11.4 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "google-pdf.jsonl\n",
    "253 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "gov.my.jsonl\n",
    "682 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "govdocs.jsonl\n",
    "136 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "hansard.jsonl\n",
    "506 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "hardwarezone-sg.jsonl\n",
    "4.79 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "hargaemas.my.jsonl\n",
    "2.47 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "hellodoktor.com.jsonl\n",
    "1.14 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "hijabista.jsonl\n",
    "22 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "hostingmalaya.com.jsonl\n",
    "393 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "hype.my.jsonl\n",
    "36.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "i-fiqh-akta.jsonl\n",
    "135 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ideasaham.my.jsonl\n",
    "97.2 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "iium-confession.jsonl\n",
    "126 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "iluminasi.jsonl\n",
    "44.6 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "imetech.com.my.jsonl\n",
    "878 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "impiana-my.jsonl\n",
    "27.9 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "inreallife.jsonl\n",
    "7.74 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "intraday.my.jsonl\n",
    "21.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "isaham.my.jsonl\n",
    "2.07 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "jomgaming.my.jsonl\n",
    "14.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "kakuchopurei.com.jsonl\n",
    "43.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "kamusbm.jsonl\n",
    "8.1 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "kebuna.com.jsonl\n",
    "274 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "kebunbandar.com.jsonl\n",
    "515 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "keluarga.jsonl\n",
    "34.1 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "kimchidaily.my.jsonl\n",
    "12.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "kisahdunia.com.jsonl\n",
    "47.8 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "klgadgetguy.com.jsonl\n",
    "20.5 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "kopiandproperty.com.jsonl\n",
    "17.8 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "leaazleeya.jsonl\n",
    "2.3 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "lipstiq.jsonl\n",
    "11.2 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "litefinance.org.jsonl\n",
    "20 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "lobakmerah.com.jsonl\n",
    "69.3 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "lom.agc.gov.my.jsonl\n",
    "25 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "lowyat.jsonl\n",
    "5.83 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "madreshoy.com.jsonl\n",
    "32.4 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "mahersaham.com.jsonl\n",
    "2.51 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "majalah-com.jsonl\n",
    "535 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "majalahpama.my.jsonl\n",
    "27.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "majalahsains.jsonl\n",
    "12.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "makanbola.jsonl\n",
    "5.78 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "maktabahalbakri.com.jsonl\n",
    "35.5 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "malay-tweets.jsonl\n",
    "1.05 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "malaykord.com.jsonl\n",
    "45.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "malaysiastock.biz.jsonl\n",
    "2.28 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "maskulin.jsonl\n",
    "18.6 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "mat-gaming.jsonl\n",
    "160 kB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "maukerja.my.jsonl\n",
    "102 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "mediahiburan.my.jsonl\n",
    "37.4 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "mingguanwanita.jsonl\n",
    "27.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "mufti_negeri_sem_artikel.jsonl\n",
    "191 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "mufti_pahang_artikel.jsonl\n",
    "72.5 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "mufti_perlis_artikel.jsonl\n",
    "320 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "mufti_wilayah_articles.jsonl\n",
    "20.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "my.theasianparent.com.jsonl\n",
    "6.24 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "myartis.com.jsonl\n",
    "41.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "mycarforum.jsonl\n",
    "Unsafe\n",
    "1.96 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "mygameon.my.jsonl\n",
    "10.4 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "myhadith.islam.gov.my.jsonl\n",
    "2.42 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "myresipi.com.jsonl\n",
    "4.51 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "nasilemaktech.com.jsonl\n",
    "13.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "news.jsonl\n",
    "6.41 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "nona.jsonl\n",
    "26.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "nurulzayani.jsonl\n",
    "6.88 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "ohbulan.com.jsonl\n",
    "81.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "ohmedia.jsonl\n",
    "16.6 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "ohmyhome.com.jsonl\n",
    "259 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ohsem.me.jsonl\n",
    "15.8 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "pandangan-hukum.jsonl\n",
    "2.62 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "pandangan-pakar.jsonl\n",
    "36.2 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "parlimen-gov.jsonl\n",
    "450 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "patriots.jsonl\n",
    "68.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "paultan-bm.jsonl\n",
    "51 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "pdfdrive.jsonl\n",
    "2.2 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "piston.my.jsonl\n",
    "22.5 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "pokde.net.jsonl\n",
    "43.1 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "politikus.jsonl\n",
    "130 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "propcafe.net.jsonl\n",
    "3.5 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "quola.my.jsonl\n",
    "2.49 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "raiz.com.my.jsonl\n",
    "898 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "realestatemy.com.jsonl\n",
    "715 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "reddit.jsonl\n",
    "143 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "relevan.jsonl\n",
    "3.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "ricebowl.my.jsonl\n",
    "1.55 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ringgitohringgit.com.jsonl\n",
    "4.41 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ringgitplus.com.jsonl\n",
    "2.93 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "rotikaya.jsonl\n",
    "31.6 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "ruby.my.jsonl\n",
    "6.78 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "sabrinatajudin.com.jsonl\n",
    "2.94 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "salary-sg.jsonl\n",
    "115 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "says.com.jsonl\n",
    "80.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "semisupervised-whisper-large-v2.jsonl\n",
    "427 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "shahbudindotcom.net.jsonl\n",
    "22.7 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "siakapkeli.jsonl\n",
    "107 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "simplywall.st.jsonl\n",
    "1.18 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "sinar-syok.jsonl\n",
    "3.1 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "siraplimau.com.jsonl\n",
    "29.6 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "skyscrapercity.com.jsonl\n",
    "363 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "snapshot.jsonl\n",
    "1.32 GB\n",
    "LFS\n",
    "add snapshots\n",
    "about 2 months ago\n",
    "soalan-jawab-hukum.jsonl\n",
    "804 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "stories.my.jsonl\n",
    "2.14 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "story.motherhood.com.my.jsonl\n",
    "232 kB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "studentportal.my.jsonl\n",
    "2.38 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "suamisihat.jsonl\n",
    "1.17 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "suararisda.jsonl\n",
    "187 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "sukanz.jsonl\n",
    "6.17 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "sunahsukasakura.com.jsonl\n",
    "8.88 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "tech-critter.com.jsonl\n",
    "16.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "techinasia.com.jsonl\n",
    "1.12 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "techlagi.jsonl\n",
    "177 kB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "technave.com.jsonl\n",
    "41.3 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "techrakyat-scraped-data-fixed.jsonl\n",
    "2.7 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "tekkaus.com.jsonl\n",
    "2.56 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "thekapital.jsonl\n",
    "3.26 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "therooftalks.com.jsonl\n",
    "778 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "tvpertiwi.com.my.jsonl\n",
    "7.91 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ubat-hellodoktor.com.jsonl\n",
    "5.93 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "umminani.jsonl\n",
    "2.04 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "umpan.jsonl\n",
    "14.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "varnam.my.jsonl\n",
    "10.4 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "vocket.jsonl\n",
    "60.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "wapcar.my.jsonl\n",
    "6.92 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "wikipedia-2023-10-01.jsonl\n",
    "348 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "wikipedia-jawi.jsonl\n",
    "1.18 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "wikipedia.jsonl\n",
    "280 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "worldofbuzz.jsonl\n",
    "31.2 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "youbaby.my.jsonl\n",
    "\"\"\"\n",
    "checkout = checkout.split('\\n')\n",
    "checkout = [os.path.join('/home/ubuntu/dedup-text-dataset', l) for l in checkout if '.jsonl' in l]\n",
    "len(checkout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21687909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob('/home/ubuntu/dedup-text-dataset/*.jsonl')\n",
    "files = [f for f in files if 'jawi' not in f]\n",
    "files = list(set(files) & set(checkout))\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7be5736",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = [\n",
    "    'wikipedia-2023-10-01.jsonl',\n",
    "]\n",
    "language_related = [\n",
    "    'dictionary.jsonl',\n",
    "    'dewanbahasa-jdbp.jsonl',\n",
    "    'dialect.jsonl',\n",
    "    'kamusbm.jsonl',\n",
    "]\n",
    "gov_related = [\n",
    "    'hansard.jsonl',\n",
    "    'lom.agc.gov.my.jsonl',\n",
    "    'parlimen-gov.jsonl',\n",
    "    'data.gov.my.jsonl',\n",
    "    'mufti_wilayah_articles.jsonl',\n",
    "    'e-khutbah.jsonl',\n",
    "    'mufti_negeri_sem_artikel.jsonl',\n",
    "    'mufti_perlis_artikel.jsonl',\n",
    "    'mufti_negeri_sem_artikel.jsonl',\n",
    "    'gov.my.jsonl',\n",
    "    'edu.my.jsonl',\n",
    "]\n",
    "research_papers = [\n",
    "    'academia-edu.jsonl',\n",
    "    'eprints.jsonl',\n",
    "]\n",
    "social_media = [\n",
    "    'iium-confession.jsonl',\n",
    "    'b.cari.com.my.jsonl',\n",
    "    'semisupervised-whisper-large-v2.jsonl',\n",
    "    'lowyat.jsonl',\n",
    "    'malay-tweets.jsonl'\n",
    "]\n",
    "common_crawl = [\n",
    "    'common-crawl.jsonl',\n",
    "    'NLLB.jsonl',\n",
    "]\n",
    "buku_teks = [\n",
    "    'buku-teks.jsonl',\n",
    "    'bumigemilang.com.jsonl',\n",
    "    'tcer.my.jsonl',\n",
    "    'mysoalan.com.jsonl'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8702b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'mistralai/Mistral-7B-v0.1',\n",
    ")\n",
    "tokenizer.add_bos_token = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a42fc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(text, size = 500):\n",
    "    splitted = text.split()\n",
    "    return [' '.join(splitted[i: i + size]) for i in range(0, len(splitted), size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "399da038",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = set(wiki) | set(language_related) | set(gov_related) | set(research_papers) | set(social_media) | set(common_crawl) | set(buku_teks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4944ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = {os.path.join('/home/ubuntu/dedup-text-dataset', f) for f in combine}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b37a8cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_articles = sorted(list(set(files) - combine))\n",
    "len(online_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a32f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = open('combine-mistral-10percent.jsonl', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0727691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/dedup-text-dataset/wikipedia-2023-10-01.jsonl'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join('/home/ubuntu/dedup-text-dataset', wiki[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a8d7bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438316 /home/ubuntu/dedup-text-dataset/wikipedia-2023-10-01.jsonl\n",
      "CPU times: user 1.82 ms, sys: 9.04 ms, total: 10.9 ms\n",
      "Wall time: 183 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "!wc -l {os.path.join('/home/ubuntu/dedup-text-dataset', wiki[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb2ced19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 167 ms, sys: 79.2 ms, total: 246 ms\n",
      "Wall time: 246 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "index = 0\n",
    "with open(os.path.join('/home/ubuntu/dedup-text-dataset', wiki[0])) as fopen:\n",
    "    for l in fopen:\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4df2e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_rows(file):\n",
    "    index = 0\n",
    "    with open(file) as fopen:\n",
    "        for l in fopen:\n",
    "            index += 1\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "139986bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43831"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(total_rows(os.path.join('/home/ubuntu/dedup-text-dataset', wiki[0])) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "461d1e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43831it [00:02, 21776.08it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in wiki:\n",
    "    f = os.path.join('/home/ubuntu/dedup-text-dataset', f)\n",
    "    rows = int(total_rows(f) * 0.1)\n",
    "    index = 0\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l)['text'] + '</s>'\n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "                index += 1\n",
    "                if index > rows:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed5272ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55616 combine-mistral-10percent.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l combine-mistral-10percent.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75fcf49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54712it [00:01, 43627.18it/s]\n",
      "4577it [00:00, 11360.90it/s]\n",
      "66it [00:00, 44412.65it/s]\n",
      "34192it [00:00, 65789.93it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in language_related:\n",
    "    f = os.path.join('/home/ubuntu/dedup-text-dataset', f)\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l) + '</s>'\n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5196224c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140932it [00:10, 13778.44it/s]\n",
      "1359it [00:00, 2587.70it/s]\n",
      "1687it [00:08, 192.41it/s]\n",
      "10889it [00:17, 640.46it/s]\n",
      "1712it [00:00, 4699.38it/s]\n",
      "809it [00:00, 2561.15it/s]\n",
      "112it [00:00, 20402.26it/s]\n",
      "144it [00:00, 17392.22it/s]\n",
      "112it [00:00, 20352.76it/s]\n",
      "30055it [00:14, 2137.88it/s]\n",
      "21590it [00:13, 1589.74it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in gov_related:\n",
    "    f = os.path.join('/home/ubuntu/dedup-text-dataset', f)\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l) + '</s>'\n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b8a42b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2076it [00:00, 15122.22it/s]\n",
      "64it [00:00, 10368.71it/s]\n",
      "7it [00:00, 8532.44it/s]\n",
      "173it [00:00, 10362.81it/s]\n",
      "3373it [00:00, 34125.20it/s]\n",
      "8it [00:00, 2926.43it/s]\n",
      "4it [00:00, 3689.73it/s]\n",
      "126it [00:00, 5943.28it/s]\n",
      "967it [00:00, 13179.31it/s]\n",
      "540it [00:00, 16808.97it/s]\n",
      "442it [00:00, 19688.01it/s]\n",
      "364it [00:00, 10291.94it/s]\n",
      "158it [00:00, 8166.76it/s]\n",
      "38it [00:00, 9038.94it/s]\n",
      "1700it [00:00, 2606.49it/s]\n",
      "17it [00:00, 5020.29it/s]\n",
      "23it [00:00, 6771.18it/s]\n",
      "142it [00:00, 5889.01it/s]\n",
      "34it [00:00, 8442.74it/s]\n",
      "4it [00:00, 2892.12it/s]\n",
      "14it [00:00, 15600.49it/s]\n",
      "1167it [00:00, 9161.73it/s]\n",
      "1232it [00:00, 7718.23it/s]\n",
      "2448it [00:00, 13983.58it/s]\n",
      "94it [00:00, 4945.68it/s]\n",
      "51it [00:00, 9301.63it/s]\n",
      "350it [00:00, 8456.21it/s]\n",
      "145it [00:00, 9632.61it/s]\n",
      "628it [00:00, 14010.31it/s]\n",
      "5315it [00:00, 20272.84it/s]\n",
      "78it [00:00, 4660.47it/s]\n",
      "2441it [00:00, 4950.80it/s]\n",
      "193it [00:00, 9024.03it/s]\n",
      "15it [00:00, 5807.68it/s]\n",
      "1022it [00:00, 21586.81it/s]\n",
      "697it [00:00, 4366.02it/s]\n",
      "116it [00:00, 9566.81it/s]\n",
      "93it [00:00, 9503.94it/s]\n",
      "1616it [00:00, 12167.14it/s]\n",
      "1291it [00:00, 11166.52it/s]\n",
      "164it [00:00, 26551.35it/s]\n",
      "1it [00:00, 1747.63it/s]\n",
      "1999it [00:00, 15259.70it/s]\n",
      "574it [00:00, 16700.99it/s]\n",
      "707it [00:00, 1676.35it/s]\n",
      "125it [00:00, 501.77it/s]\n",
      "1660520it [00:28, 57791.97it/s]\n",
      "388it [00:00, 37197.48it/s]\n",
      "22it [00:00, 7639.90it/s]\n",
      "684it [00:00, 12711.31it/s]\n",
      "13it [00:00, 12796.52it/s]\n",
      "1314it [00:00, 12159.55it/s]\n",
      "1it [00:00, 1455.34it/s]\n",
      "4it [00:00, 8512.03it/s]\n",
      "844it [00:00, 8303.55it/s]\n",
      "47it [00:00, 14154.68it/s]\n",
      "350it [00:00, 5667.76it/s]\n",
      "121it [00:00, 6181.54it/s]\n",
      "1107it [00:00, 19654.47it/s]\n",
      "39it [00:00, 9461.38it/s]\n",
      "820it [00:00, 19784.11it/s]\n",
      "1644it [00:00, 12837.24it/s]\n",
      "5it [00:00, 6356.93it/s]\n",
      "10it [00:00, 5955.28it/s]\n",
      "1310it [00:00, 14451.51it/s]\n",
      "722it [00:00, 18845.76it/s]\n",
      "1886it [00:00, 14191.55it/s]\n",
      "1123it [00:00, 17469.98it/s]\n",
      "510it [00:00, 9822.77it/s]\n",
      "53it [00:00, 10023.36it/s]\n",
      "492it [00:00, 14141.40it/s]\n",
      "394it [00:00, 9547.49it/s]\n",
      "2898it [00:00, 14962.05it/s]\n",
      "675it [00:00, 9771.90it/s]\n",
      "63it [00:00, 8828.05it/s]\n",
      "188479it [00:04, 42424.70it/s]\n",
      "1791it [00:00, 25254.23it/s]\n",
      "152it [00:00, 5272.06it/s]\n",
      "344it [00:00, 20283.70it/s]\n",
      "336it [00:00, 5107.76it/s]\n",
      "1056it [00:00, 7999.29it/s]\n",
      "147382it [00:06, 24237.57it/s]\n",
      "710it [00:00, 14444.25it/s]\n",
      "4it [00:00, 23172.95it/s]\n",
      "4it [00:00, 6274.20it/s]\n",
      "1614it [00:00, 15006.22it/s]\n",
      "922it [00:00, 12713.90it/s]\n",
      "6it [00:00, 11027.97it/s]\n",
      "100it [00:00, 6329.97it/s]\n",
      "2249it [00:00, 22862.70it/s]\n",
      "579871it [00:12, 45477.89it/s]\n",
      "641it [00:00, 15963.64it/s]\n",
      "52it [00:00, 10462.62it/s]\n",
      "236it [00:00, 13432.15it/s]\n",
      "470it [00:00, 11136.22it/s]\n",
      "283934it [00:17, 15919.25it/s]\n",
      "935it [00:00, 13295.25it/s]\n",
      "114it [00:00, 6659.76it/s]\n",
      "3208it [00:00, 13794.49it/s]\n",
      "986it [00:00, 20571.15it/s]\n",
      "7it [00:00, 6022.59it/s]\n",
      "381it [00:00, 9043.75it/s]\n",
      "213it [00:00, 29611.76it/s]\n",
      "8it [00:00, 21931.00it/s]\n",
      "1048it [00:00, 6535.79it/s]\n",
      "2032it [00:00, 14488.34it/s]\n",
      "571it [00:04, 133.98it/s]\n",
      "620it [00:00, 9891.36it/s]\n",
      "1415it [00:00, 11538.61it/s]\n",
      "161it [00:00, 63299.86it/s]\n",
      "29it [00:00, 3778.65it/s]\n",
      "24it [00:00, 4977.66it/s]\n",
      "27it [00:00, 8845.98it/s]\n",
      "15it [00:00, 8399.81it/s]\n",
      "109249it [00:01, 73853.83it/s]\n",
      "124it [00:00, 12538.12it/s]\n",
      "34it [00:00, 9314.59it/s]\n",
      "54it [00:00, 4823.09it/s]\n",
      "72it [00:00, 8042.77it/s]\n",
      "2281it [00:00, 23185.64it/s]\n",
      "113it [00:00, 6942.48it/s]\n",
      "104it [00:00, 11942.06it/s]\n",
      "23020it [00:00, 46536.55it/s]\n",
      "2254it [00:00, 10583.96it/s]\n",
      "640it [00:00, 8559.56it/s]\n",
      "5130it [00:00, 16760.23it/s]\n",
      "28it [00:00, 8830.78it/s]\n",
      "307it [00:00, 29600.50it/s]\n",
      "850it [00:00, 10043.21it/s]\n",
      "62977it [00:01, 41979.28it/s]\n",
      "20917it [00:02, 7441.53it/s]\n",
      "21it [00:00, 11347.64it/s]\n",
      "107it [00:00, 15168.50it/s]\n",
      "157it [00:00, 64307.20it/s]\n",
      "75it [00:00, 11989.21it/s]\n",
      "10it [00:00, 3624.53it/s]\n",
      "9it [00:00, 11322.36it/s]\n",
      "357it [00:00, 18926.93it/s]\n",
      "412it [00:00, 18023.08it/s]\n",
      "459it [00:00, 10056.76it/s]\n",
      "30it [00:00, 9073.34it/s]\n",
      "4it [00:00, 6345.39it/s]\n",
      "2504it [00:00, 20319.57it/s]\n",
      "22it [00:00, 3127.11it/s]\n",
      "64it [00:00, 14434.34it/s]\n",
      "59it [00:00, 7124.13it/s]\n",
      "16it [00:00, 8318.94it/s]\n",
      "362it [00:00, 19474.36it/s]\n",
      "77it [00:00, 5898.84it/s]\n",
      "71it [00:00, 11735.32it/s]\n",
      "540it [00:00, 12791.41it/s]\n",
      "383it [00:00, 12309.81it/s]\n",
      "2507it [00:00, 13436.84it/s]\n",
      "1131it [00:00, 37719.52it/s]\n",
      "1527it [00:00, 15500.32it/s]\n",
      "8it [00:00, 5476.49it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in online_articles:\n",
    "    rows = int(total_rows(f) * 0.1)\n",
    "    index = 0\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l) + '</s>'\n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "                index += 1\n",
    "                if index > rows:\n",
    "                    break\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c67ace6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "buku_teks = [\n",
    "    'buku-teks.jsonl',\n",
    "    'bumigemilang.com.jsonl',\n",
    "    'mysoalan.com.jsonl',\n",
    "    'tcer.my.jsonl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22688d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in buku_teks:\n",
    "    f = os.path.join('/home/ubuntu/dedup-text-dataset', f)\n",
    "    with open(f) as fopen:\n",
    "        for l in fopen:\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l) + '</s>'\n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2062865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_files = [\n",
    "    '/home/ubuntu/code_instructions_120k.jsonl.requested', \n",
    "    '/home/ubuntu/python_evol_instruct_51k.jsonl.requested',\n",
    "    '/home/ubuntu/unnatural-instructions.jsonl.requested',\n",
    "    '/home/ubuntu/glaive_coder_raw_text.jsonl.requested',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcce00d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12162it [00:00, 39439.16it/s]\n",
      "5339it [00:00, 17918.98it/s]\n",
      "17959it [00:00, 22774.18it/s]\n",
      "14559it [00:00, 23063.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in code_files:\n",
    "    rows = int(total_rows(f) * 0.1)\n",
    "    index = 0\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            l = json.loads(l)['r']\n",
    "            \n",
    "            if '```' not in l:\n",
    "                continue\n",
    "                \n",
    "            data = '<s>' + l + '</s>'\n",
    "            partitioned = partition(data)\n",
    "            for p in partitioned:\n",
    "                data = {\n",
    "                    'text': p,\n",
    "                }\n",
    "                a.write(f'{json.dumps(data)}\\n')\n",
    "                a.flush()\n",
    "            index += 1\n",
    "            if index > rows:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d7eff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61162it [00:01, 30795.96it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/home/ubuntu/WizardLM_evol_instruct_V2_143k.translated.jsonl') as fopen:\n",
    "    rows = int(total_rows(f) * 0.1)\n",
    "    index = 0\n",
    "    for l in tqdm(fopen):\n",
    "        l = json.loads(l)\n",
    "        conversations = l['conversations']\n",
    "        conversations = [c.get('value_ms') for c in conversations]\n",
    "        if any([c is None for c in conversations]):\n",
    "            continue\n",
    "        l = ' '.join(conversations)\n",
    "        if '```' not in l:\n",
    "            continue\n",
    "        data = '<s>' + l + '</s>'\n",
    "        partitioned = partition(data)\n",
    "        for p in partitioned:\n",
    "            data = {\n",
    "                'text': p,\n",
    "            }\n",
    "            a.write(f'{json.dumps(data)}\\n')\n",
    "            a.flush()\n",
    "        index += 1\n",
    "        if index > rows:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c736de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/translated-gpt4all-code.json') as fopen:\n",
    "    code = json.load(fopen)\n",
    "    \n",
    "rows = int(len(code) * 0.1)\n",
    "index = 0\n",
    "for c in code:\n",
    "    if len(c['prompt']) != 2:\n",
    "        continue\n",
    "    if len(c['response']) != 2:\n",
    "        continue\n",
    "    l = ' '.join([c['prompt'][1], c['response'][1]])\n",
    "    data = '<s>' + l + '</s>'\n",
    "    partitioned = partition(data)\n",
    "    for p in partitioned:\n",
    "        data = {\n",
    "            'text': p,\n",
    "        }\n",
    "        a.write(f'{json.dumps(data)}\\n')\n",
    "        a.flush()\n",
    "    \n",
    "    index += 1\n",
    "    if index > rows:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1482b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = int(total_rows('math-qa.jsonl.translated') * 0.1)\n",
    "index = 0\n",
    "\n",
    "with open('math-qa.jsonl.translated') as fopen:\n",
    "    for l in fopen:\n",
    "        l = json.loads(l)\n",
    "        l = [l['Problem_ms'], l['options_ms'], l['Rationale_ms']]\n",
    "        if any([l_ is None for l_ in l]):\n",
    "            continue\n",
    "        l = ' '.join(l)\n",
    "        data = '<s>' + l + '</s>'\n",
    "        partitioned = partition(data)\n",
    "        for p in partitioned:\n",
    "            data = {\n",
    "                'text': p,\n",
    "            }\n",
    "            a.write(f'{json.dumps(data)}\\n')\n",
    "            a.flush()\n",
    "        index += 1\n",
    "        if index > rows:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f7b7d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2291it [00:00, 13819.97it/s]\n"
     ]
    }
   ],
   "source": [
    "rows = int(total_rows('mini-math23k.jsonl.requested') * 0.1)\n",
    "index = 0\n",
    "\n",
    "with open('mini-math23k.jsonl.requested') as fopen:\n",
    "    for l in tqdm(fopen):\n",
    "        l = json.loads(l)\n",
    "        data = '<s>' + l['r']['result'] + '</s>'\n",
    "        partitioned = partition(data)\n",
    "        for p in partitioned:\n",
    "            data = {\n",
    "                'text': p,\n",
    "            }\n",
    "            a.write(f'{json.dumps(data)}\\n')\n",
    "            a.flush()\n",
    "        \n",
    "        index += 1\n",
    "        if index > rows:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f39a568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25650it [00:00, 31823.46it/s]\n"
     ]
    }
   ],
   "source": [
    "rows = int(total_rows('math-instruct.jsonl') * 0.1)\n",
    "index = 0\n",
    "\n",
    "with open('math-instruct.jsonl') as fopen:\n",
    "    for l in tqdm(fopen):\n",
    "        l = json.loads(l)\n",
    "        data = '<s>' + l['r']['result'] + '</s>'\n",
    "        partitioned = partition(data)\n",
    "        for p in partitioned:\n",
    "            data = {\n",
    "                'text': p,\n",
    "            }\n",
    "            a.write(f'{json.dumps(data)}\\n')\n",
    "            a.flush()\n",
    "        \n",
    "        index += 1\n",
    "        if index > rows:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "332af17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected = [\n",
    "    'markah untuk setiap satu',\n",
    "    'soalan mesti dijawab dalam',\n",
    "    '25 markah',\n",
    "    '50 markah'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19f3dba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [00:00, 92.34it/s] \n",
      "19111it [00:11, 1640.85it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in research_papers:\n",
    "    f = os.path.join('/home/ubuntu/dedup-text-dataset', f)\n",
    "    rows = int(total_rows(f) * 0.1)\n",
    "    index = 0\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l) + '</s>'\n",
    "                    \n",
    "                data_lower = data.lower()\n",
    "                if any([r in data_lower for r in rejected]):\n",
    "                    continue\n",
    "                    \n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "                    \n",
    "                index += 1\n",
    "                if index > rows:\n",
    "                    break\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e04c1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "376b3355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 ubuntu ubuntu 6.3G Oct 30 01:40 combine-mistral-10percent.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh combine-mistral-10percent.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
