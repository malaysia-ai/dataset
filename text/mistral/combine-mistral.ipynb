{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19ccc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/mesolitica/translated-unnatural_code_instructions_20M/resolve/main/unnatural-instructions.jsonl.requested\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-python-evol-instruct-51k/resolve/main/python_evol_instruct_51k.jsonl.requested\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-code-instructions-122k/resolve/main/code_instructions_120k.jsonl.requested\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-glaive_coder_raw_text/resolve/main/glaive_coder_raw_text.jsonl.requested\n",
    "# !wget https://huggingface.co/datasets/mesolitica/chatgpt-gpt4all/resolve/main/translated-gpt4all-code.json\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-WizardLM_evol_instruct_V2_196k/resolve/main/WizardLM_evol_instruct_V2_143k.translated.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4010216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/mesolitica/translated-math_qa/resolve/main/math-qa.jsonl.translated\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-mini-math23k-v1/resolve/main/mini-math23k.jsonl.requested\n",
    "# !wget https://huggingface.co/datasets/mesolitica/translated-MathInstruct/resolve/main/math-instruct.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9120317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/malaysia-ai/dedup-text-dataset/resolve/main/buku-teks.jsonl\n",
    "# !wget https://huggingface.co/datasets/malaysia-ai/dedup-text-dataset/resolve/main/bumigemilang.com.jsonl\n",
    "# !wget https://huggingface.co/datasets/malaysia-ai/dedup-text-dataset/resolve/main/tcer.my.jsonl\n",
    "# !wget https://huggingface.co/datasets/malaysia-ai/dedup-text-dataset/resolve/main/mysoalan.com.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867b4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://huggingface.co/datasets/malaysia-ai/dedup-text-dataset\n",
    "# !cd dedup-text-dataset && git checkout 4e0a81699c05c3b52e5845142faa2ee22c1fcb9e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "007bfe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a53f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkout = \"\"\"\n",
    "NLLB.jsonl\n",
    "2.44 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "academia-edu.jsonl\n",
    "260 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "agendadaily.jsonl\n",
    "50.1 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ajar.jsonl\n",
    "2.58 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "akuislam.com.jsonl\n",
    "1.01 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "aliffchannel.jsonl\n",
    "6.19 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "amanz.jsonl\n",
    "43.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "article.poliklinikazzaara.com.my.jsonl\n",
    "282 kB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "artikel.jsonl\n",
    "252 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "asklegal.jsonl\n",
    "8.36 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "autobuzz.jsonl\n",
    "23.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "azhafizah.com.jsonl\n",
    "13.1 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "b.cari.com.my.jsonl\n",
    "1.45 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "beautifulnara.jsonl\n",
    "7.07 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "bidadari.jsonl\n",
    "14.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "bjak.my.jsonl\n",
    "9.8 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "blog.fincrew.my.jsonl\n",
    "1.48 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "blog.limkitsiang.com.jsonl\n",
    "94.7 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "blog.malaysia-asia.jsonl\n",
    "1.24 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "blog.pandai.com.jsonl\n",
    "916 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "blogmalaysia-com.jsonl\n",
    "8.2 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "blogtipskerjaya.net.jsonl\n",
    "4.14 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "bullishbursa.blogspot.com.jsonl\n",
    "137 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "bumiinvest20.home.blog.jsonl\n",
    "298 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "buro247.jsonl\n",
    "46 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "carlist-my.jsonl\n",
    "36 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "carsifu-my.jsonl\n",
    "61 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "carsome-my.jsonl\n",
    "7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "columbiaasia.com.jsonl\n",
    "2.13 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "common-crawl.jsonl\n",
    "2.35 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "data.gov.my.jsonl\n",
    "1.03 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "denaihati.jsonl\n",
    "16.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "dewanbahasa-jdbp.jsonl\n",
    "17 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "dialect.jsonl\n",
    "16.4 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "dictionary.jsonl\n",
    "28 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "discoverkl.jsonl\n",
    "6.11 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "diva-my.jsonl\n",
    "14.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "doctoroncall.jsonl\n",
    "87 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "dotproperty.com.my.jsonl\n",
    "2.65 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "dsf-my.jsonl\n",
    "99.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "e-khutbah.jsonl\n",
    "27.7 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ecentral.my.jsonl\n",
    "8.75 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "edu.my.jsonl\n",
    "531 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "ekonomirakyat.com.jsonl\n",
    "886 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "eprints.jsonl\n",
    "5.2 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ering.jsonl\n",
    "16.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "fiksyenshasha.jsonl\n",
    "78.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "fintechnews.my.jsonl\n",
    "3.73 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "fuh.my.jsonl\n",
    "4.08 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "gamerbraves.com.jsonl\n",
    "44.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "gamersantai.com.jsonl\n",
    "44.3 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "gamersonduty.com.jsonl\n",
    "1.97 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "garis-panduan.jsonl\n",
    "237 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "gempak.com.jsonl\n",
    "50.3 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "goodymy.com.jsonl\n",
    "11.4 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "google-pdf.jsonl\n",
    "253 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "gov.my.jsonl\n",
    "682 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "govdocs.jsonl\n",
    "136 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "hansard.jsonl\n",
    "506 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "hardwarezone-sg.jsonl\n",
    "4.79 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "hargaemas.my.jsonl\n",
    "2.47 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "hellodoktor.com.jsonl\n",
    "1.14 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "hijabista.jsonl\n",
    "22 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "hostingmalaya.com.jsonl\n",
    "393 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "hype.my.jsonl\n",
    "36.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "i-fiqh-akta.jsonl\n",
    "135 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ideasaham.my.jsonl\n",
    "97.2 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "iium-confession.jsonl\n",
    "126 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "iluminasi.jsonl\n",
    "44.6 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "imetech.com.my.jsonl\n",
    "878 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "impiana-my.jsonl\n",
    "27.9 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "inreallife.jsonl\n",
    "7.74 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "intraday.my.jsonl\n",
    "21.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "isaham.my.jsonl\n",
    "2.07 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "jomgaming.my.jsonl\n",
    "14.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "kakuchopurei.com.jsonl\n",
    "43.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "kamusbm.jsonl\n",
    "8.1 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "kebuna.com.jsonl\n",
    "274 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "kebunbandar.com.jsonl\n",
    "515 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "keluarga.jsonl\n",
    "34.1 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "kimchidaily.my.jsonl\n",
    "12.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "kisahdunia.com.jsonl\n",
    "47.8 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "klgadgetguy.com.jsonl\n",
    "20.5 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "kopiandproperty.com.jsonl\n",
    "17.8 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "leaazleeya.jsonl\n",
    "2.3 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "lipstiq.jsonl\n",
    "11.2 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "litefinance.org.jsonl\n",
    "20 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "lobakmerah.com.jsonl\n",
    "69.3 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "lom.agc.gov.my.jsonl\n",
    "25 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "lowyat.jsonl\n",
    "5.83 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "madreshoy.com.jsonl\n",
    "32.4 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "mahersaham.com.jsonl\n",
    "2.51 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "majalah-com.jsonl\n",
    "535 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "majalahpama.my.jsonl\n",
    "27.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "majalahsains.jsonl\n",
    "12.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "makanbola.jsonl\n",
    "5.78 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "maktabahalbakri.com.jsonl\n",
    "35.5 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "malay-tweets.jsonl\n",
    "1.05 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "malaykord.com.jsonl\n",
    "45.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "malaysiastock.biz.jsonl\n",
    "2.28 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "maskulin.jsonl\n",
    "18.6 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "mat-gaming.jsonl\n",
    "160 kB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "maukerja.my.jsonl\n",
    "102 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "mediahiburan.my.jsonl\n",
    "37.4 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "mingguanwanita.jsonl\n",
    "27.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "mufti_negeri_sem_artikel.jsonl\n",
    "191 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "mufti_pahang_artikel.jsonl\n",
    "72.5 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "mufti_perlis_artikel.jsonl\n",
    "320 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "mufti_wilayah_articles.jsonl\n",
    "20.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "my.theasianparent.com.jsonl\n",
    "6.24 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "myartis.com.jsonl\n",
    "41.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "mycarforum.jsonl\n",
    "Unsafe\n",
    "1.96 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "mygameon.my.jsonl\n",
    "10.4 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "myhadith.islam.gov.my.jsonl\n",
    "2.42 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "myresipi.com.jsonl\n",
    "4.51 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "nasilemaktech.com.jsonl\n",
    "13.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "news.jsonl\n",
    "6.41 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "nona.jsonl\n",
    "26.8 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "nurulzayani.jsonl\n",
    "6.88 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "ohbulan.com.jsonl\n",
    "81.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "ohmedia.jsonl\n",
    "16.6 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "ohmyhome.com.jsonl\n",
    "259 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ohsem.me.jsonl\n",
    "15.8 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "pandangan-hukum.jsonl\n",
    "2.62 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "pandangan-pakar.jsonl\n",
    "36.2 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "parlimen-gov.jsonl\n",
    "450 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "patriots.jsonl\n",
    "68.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "paultan-bm.jsonl\n",
    "51 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "pdfdrive.jsonl\n",
    "2.2 GB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "piston.my.jsonl\n",
    "22.5 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "pokde.net.jsonl\n",
    "43.1 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "politikus.jsonl\n",
    "130 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "propcafe.net.jsonl\n",
    "3.5 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "quola.my.jsonl\n",
    "2.49 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "raiz.com.my.jsonl\n",
    "898 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "realestatemy.com.jsonl\n",
    "715 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "reddit.jsonl\n",
    "143 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "relevan.jsonl\n",
    "3.9 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "ricebowl.my.jsonl\n",
    "1.55 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ringgitohringgit.com.jsonl\n",
    "4.41 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ringgitplus.com.jsonl\n",
    "2.93 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "rotikaya.jsonl\n",
    "31.6 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "ruby.my.jsonl\n",
    "6.78 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "sabrinatajudin.com.jsonl\n",
    "2.94 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "salary-sg.jsonl\n",
    "115 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "says.com.jsonl\n",
    "80.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "semisupervised-whisper-large-v2.jsonl\n",
    "427 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "shahbudindotcom.net.jsonl\n",
    "22.7 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "siakapkeli.jsonl\n",
    "107 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "simplywall.st.jsonl\n",
    "1.18 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "sinar-syok.jsonl\n",
    "3.1 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "siraplimau.com.jsonl\n",
    "29.6 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "skyscrapercity.com.jsonl\n",
    "363 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "snapshot.jsonl\n",
    "1.32 GB\n",
    "LFS\n",
    "add snapshots\n",
    "about 2 months ago\n",
    "soalan-jawab-hukum.jsonl\n",
    "804 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "stories.my.jsonl\n",
    "2.14 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "story.motherhood.com.my.jsonl\n",
    "232 kB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "studentportal.my.jsonl\n",
    "2.38 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "suamisihat.jsonl\n",
    "1.17 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "suararisda.jsonl\n",
    "187 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "sukanz.jsonl\n",
    "6.17 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "sunahsukasakura.com.jsonl\n",
    "8.88 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "tech-critter.com.jsonl\n",
    "16.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "techinasia.com.jsonl\n",
    "1.12 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "techlagi.jsonl\n",
    "177 kB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "technave.com.jsonl\n",
    "41.3 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "techrakyat-scraped-data-fixed.jsonl\n",
    "2.7 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "tekkaus.com.jsonl\n",
    "2.56 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "thekapital.jsonl\n",
    "3.26 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "therooftalks.com.jsonl\n",
    "778 kB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "tvpertiwi.com.my.jsonl\n",
    "7.91 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "ubat-hellodoktor.com.jsonl\n",
    "5.93 MB\n",
    "LFS\n",
    "add data\n",
    "about 1 month ago\n",
    "umminani.jsonl\n",
    "2.04 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "umpan.jsonl\n",
    "14.7 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "varnam.my.jsonl\n",
    "10.4 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "vocket.jsonl\n",
    "60.5 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "wapcar.my.jsonl\n",
    "6.92 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "wikipedia-2023-10-01.jsonl\n",
    "348 MB\n",
    "LFS\n",
    "add data\n",
    "4 days ago\n",
    "wikipedia-jawi.jsonl\n",
    "1.18 GB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "wikipedia.jsonl\n",
    "280 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "worldofbuzz.jsonl\n",
    "31.2 MB\n",
    "LFS\n",
    "add dataset\n",
    "about 2 months ago\n",
    "youbaby.my.jsonl\n",
    "\"\"\"\n",
    "checkout = checkout.split('\\n')\n",
    "checkout = [os.path.join('/home/ubuntu/dedup-text-dataset', l) for l in checkout if '.jsonl' in l]\n",
    "len(checkout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d35e185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e140a8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob('/home/ubuntu/dedup-text-dataset/*.jsonl')\n",
    "files = [f for f in files if 'jawi' not in f]\n",
    "files = list(set(files) & set(checkout))\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df37171",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = [\n",
    "    'wikipedia-2023-10-01.jsonl',\n",
    "]\n",
    "language_related = [\n",
    "    'dictionary.jsonl',\n",
    "    'dewanbahasa-jdbp.jsonl',\n",
    "    'dialect.jsonl',\n",
    "    'kamusbm.jsonl',\n",
    "]\n",
    "gov_related = [\n",
    "    'hansard.jsonl',\n",
    "    'lom.agc.gov.my.jsonl',\n",
    "    'parlimen-gov.jsonl',\n",
    "    'data.gov.my.jsonl',\n",
    "    'mufti_wilayah_articles.jsonl',\n",
    "    'e-khutbah.jsonl',\n",
    "    'mufti_negeri_sem_artikel.jsonl',\n",
    "    'mufti_perlis_artikel.jsonl',\n",
    "    'mufti_negeri_sem_artikel.jsonl',\n",
    "    'gov.my.jsonl',\n",
    "    'edu.my.jsonl',\n",
    "]\n",
    "research_papers = [\n",
    "    'academia-edu.jsonl',\n",
    "    'eprints.jsonl',\n",
    "]\n",
    "social_media = [\n",
    "    'iium-confession.jsonl',\n",
    "    'b.cari.com.my.jsonl',\n",
    "    'semisupervised-whisper-large-v2.jsonl',\n",
    "    'lowyat.jsonl',\n",
    "    'malay-tweets.jsonl'\n",
    "]\n",
    "common_crawl = [\n",
    "    'common-crawl.jsonl',\n",
    "    'NLLB.jsonl',\n",
    "]\n",
    "buku_teks = [\n",
    "    'buku-teks.jsonl',\n",
    "    'bumigemilang.com.jsonl',\n",
    "    'tcer.my.jsonl',\n",
    "    'mysoalan.com.jsonl'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1af22753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'mistralai/Mistral-7B-v0.1',\n",
    ")\n",
    "tokenizer.add_bos_token = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57829912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 6312, 28709, 2], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('<s>hello</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e71bdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(text, size = 500):\n",
    "    splitted = text.split()\n",
    "    return [' '.join(splitted[i: i + size]) for i in range(0, len(splitted), size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9068329",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = set(wiki) | set(language_related) | set(gov_related) | set(research_papers) | set(social_media) | set(common_crawl) | set(buku_teks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fe7b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = {os.path.join('/home/ubuntu/dedup-text-dataset', f) for f in combine}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66b8441a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "online_articles = sorted(list(set(files) - combine))\n",
    "len(online_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41f4b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = open('combine-mistral.jsonl', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14fc36e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "438316it [00:09, 46919.69it/s] \n"
     ]
    }
   ],
   "source": [
    "for f in wiki:\n",
    "    f = os.path.join('/home/ubuntu/dedup-text-dataset', f)\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l) + '</s>'\n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3507d7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54712it [00:01, 29048.20it/s]\n",
      "4577it [00:00, 7538.81it/s]\n",
      "66it [00:00, 11956.81it/s]\n",
      "34192it [00:00, 43422.78it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in language_related:\n",
    "    f = os.path.join('/home/ubuntu/dedup-text-dataset', f)\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l) + '</s>'\n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c57e119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140932it [00:32, 4338.88it/s]\n",
      "1359it [00:01, 1253.25it/s]\n",
      "1687it [00:22, 73.67it/s] \n",
      "10889it [01:01, 177.42it/s]\n",
      "1712it [00:00, 2097.91it/s]\n",
      "809it [00:00, 859.63it/s]\n",
      "112it [00:00, 4395.11it/s]\n",
      "144it [00:00, 5092.71it/s]\n",
      "112it [00:00, 13742.56it/s]\n",
      "30055it [00:52, 577.73it/s] \n",
      "21590it [00:27, 791.39it/s] \n"
     ]
    }
   ],
   "source": [
    "for f in gov_related:\n",
    "    f = os.path.join('/home/ubuntu/dedup-text-dataset', f)\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l) + '</s>'\n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "767cd66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20763it [00:02, 9177.01it/s] \n",
      "640it [00:00, 7027.40it/s]\n",
      "74it [00:00, 1623.79it/s]\n",
      "1733it [00:00, 6905.46it/s]\n",
      "33730it [00:02, 16013.09it/s]\n",
      "87it [00:00, 6039.66it/s]\n",
      "47it [00:00, 2899.30it/s]\n",
      "1263it [00:00, 3798.53it/s]\n",
      "9672it [00:01, 9253.15it/s]\n",
      "5400it [00:00, 7576.01it/s]\n",
      "4425it [00:00, 15242.26it/s]\n",
      "3645it [00:00, 7443.74it/s]\n",
      "1589it [00:00, 4230.47it/s]\n",
      "385it [00:00, 5593.39it/s]\n",
      "17004it [00:04, 3541.26it/s]\n",
      "176it [00:00, 4637.24it/s]\n",
      "233it [00:00, 5103.44it/s]\n",
      "1427it [00:00, 5422.58it/s]\n",
      "341it [00:00, 2359.49it/s]\n",
      "46it [00:00, 3222.18it/s]\n",
      "140it [00:00, 5794.90it/s]\n",
      "11678it [00:01, 6262.70it/s]\n",
      "12326it [00:01, 8483.62it/s]\n",
      "24482it [00:02, 9379.55it/s]\n",
      "947it [00:00, 2129.32it/s]\n",
      "512it [00:00, 7247.18it/s]\n",
      "3506it [00:00, 6054.59it/s]\n",
      "1459it [00:00, 6739.99it/s]\n",
      "6285it [00:00, 10030.51it/s]\n",
      "53158it [00:03, 16664.30it/s]\n",
      "788it [00:00, 6859.88it/s]\n",
      "24411it [00:03, 6276.00it/s]\n",
      "1932it [00:00, 6586.12it/s]\n",
      "151it [00:00, 4171.43it/s]\n",
      "10229it [00:00, 14518.08it/s]\n",
      "6976it [00:08, 833.89it/s] \n",
      "1160it [00:00, 1243.15it/s]\n",
      "937it [00:01, 470.01it/s]\n",
      "16169it [00:04, 3980.13it/s]\n",
      "12914it [00:02, 4764.40it/s]\n",
      "1647it [00:00, 17206.30it/s]\n",
      "12it [00:00, 1185.50it/s]\n",
      "19992it [00:08, 2467.15it/s]\n",
      "5747it [00:02, 2373.16it/s]\n",
      "7072it [00:44, 158.76it/s]\n",
      "1252it [00:21, 59.01it/s] \n",
      "16605209it [09:55, 27880.75it/s]\n",
      "3888it [00:00, 25468.18it/s]\n",
      "223it [00:00, 4405.89it/s]\n",
      "6848it [00:00, 7996.60it/s]\n",
      "139it [00:00, 5139.35it/s]\n",
      "13145it [00:01, 8439.13it/s]\n",
      "13it [00:00, 1100.91it/s]\n",
      "42it [00:00, 4960.60it/s]\n",
      "8447it [00:01, 5522.75it/s]\n",
      "473it [00:00, 9549.39it/s]\n",
      "3502it [00:01, 3019.18it/s]\n",
      "1216it [00:00, 4099.24it/s]\n",
      "11076it [00:00, 13193.16it/s]\n",
      "391it [00:00, 3665.55it/s]\n",
      "8200it [00:00, 12944.48it/s]\n",
      "16443it [00:02, 7826.46it/s]\n",
      "58it [00:00, 2200.82it/s]\n",
      "100it [00:00, 2749.07it/s]\n",
      "13105it [00:01, 9360.45it/s]\n",
      "7226it [00:00, 12874.18it/s]\n",
      "18860it [00:01, 9579.24it/s]\n",
      "11236it [00:05, 2110.66it/s]\n",
      "5103it [00:04, 1171.64it/s]\n",
      "532it [00:00, 4548.61it/s]\n",
      "4921it [00:00, 9917.92it/s] \n",
      "3948it [00:05, 703.52it/s] \n",
      "28986it [00:05, 5770.96it/s]\n",
      "6751it [00:01, 5853.41it/s]\n",
      "638it [00:00, 5977.15it/s]\n",
      "1884790it [00:50, 37583.24it/s]\n",
      "17918it [00:01, 9546.82it/s] \n",
      "1523it [00:00, 4161.23it/s]\n",
      "3442it [00:00, 14531.20it/s]\n",
      "3365it [00:01, 2823.59it/s]\n",
      "10565it [00:02, 4968.50it/s]\n",
      "1473823it [01:34, 15643.17it/s]\n",
      "7106it [00:00, 10139.15it/s]\n",
      "49it [00:00, 2556.74it/s]\n",
      "40it [00:00, 4058.74it/s]\n",
      "16144it [00:01, 9781.92it/s] \n",
      "9221it [00:01, 8839.38it/s]\n",
      "68it [00:00, 7896.91it/s]\n",
      "1006it [00:00, 4230.66it/s]\n",
      "22491it [00:01, 13083.91it/s]\n",
      "5798716it [02:55, 32965.28it/s]\n",
      "6418it [00:00, 10927.69it/s]\n",
      "523it [00:00, 6153.30it/s]\n",
      "2368it [00:00, 13190.53it/s]\n",
      "4702it [00:00, 8384.31it/s]\n",
      "2839348it [05:17, 8951.47it/s] \n",
      "9359it [00:00, 9662.52it/s] \n",
      "1143it [00:00, 2057.33it/s]\n",
      "32085it [00:05, 5418.70it/s]\n",
      "9865it [00:03, 3188.60it/s]\n",
      "70it [00:00, 3532.00it/s]\n",
      "3819it [00:00, 5050.75it/s]\n",
      "2132it [00:00, 15795.02it/s]\n",
      "89it [00:00, 6550.84it/s]\n",
      "10483it [00:02, 4849.79it/s]\n",
      "20320it [00:02, 9349.01it/s] \n",
      "5714it [01:14, 76.98it/s] \n",
      "6204it [00:03, 1801.39it/s]\n",
      "14158it [00:01, 8230.48it/s]\n",
      "1619it [00:00, 1685.46it/s]\n",
      "298it [00:00, 1538.89it/s]\n",
      "240it [00:00, 2247.39it/s]\n",
      "276it [00:00, 5738.67it/s]\n",
      "153it [00:00, 2228.53it/s]\n",
      "1092497it [00:26, 41472.90it/s]\n",
      "1245it [00:00, 8870.04it/s]\n",
      "348it [00:00, 5069.24it/s]\n",
      "547it [00:00, 2858.90it/s]\n",
      "726it [00:00, 5662.89it/s]\n",
      "22818it [00:01, 16057.56it/s]\n",
      "1138it [00:00, 5319.54it/s]\n",
      "1045it [00:00, 9209.77it/s]\n",
      "230208it [00:07, 30972.38it/s]\n",
      "22542it [00:03, 7345.33it/s]\n",
      "6400it [00:06, 1018.71it/s]\n",
      "51302it [00:05, 10217.74it/s]\n",
      "286it [00:00, 7779.82it/s]\n",
      "3071it [00:00, 19934.89it/s]\n",
      "8504it [00:01, 7731.66it/s]\n",
      "629779it [00:26, 24013.71it/s]\n",
      "209173it [00:46, 4504.26it/s] \n",
      "216it [00:00, 3429.29it/s]\n",
      "1074it [00:00, 4694.19it/s]\n",
      "1578it [00:00, 37397.30it/s]\n",
      "753it [00:00, 9153.86it/s]\n",
      "101it [00:00, 1250.42it/s]\n",
      "91it [00:00, 6110.92it/s]\n",
      "3578it [00:00, 11160.50it/s]\n",
      "4122it [00:00, 9189.51it/s] \n",
      "4591it [00:00, 6965.01it/s]\n",
      "300it [00:00, 6023.79it/s]\n",
      "48it [00:00, 3112.42it/s]\n",
      "25040it [00:02, 11705.65it/s]\n",
      "220it [00:00, 1760.50it/s]\n",
      "644it [00:00, 6946.86it/s]\n",
      "597it [00:00, 5647.65it/s]\n",
      "168it [00:00, 4190.09it/s]\n",
      "3626it [00:00, 10656.56it/s]\n",
      "772it [00:00, 3892.82it/s]\n",
      "711it [00:00, 9225.78it/s]\n",
      "5407it [00:00, 10656.21it/s]\n",
      "3834it [00:00, 8341.39it/s]\n",
      "25073it [00:02, 10222.88it/s]\n",
      "11314it [00:00, 26149.77it/s]\n",
      "15277it [00:01, 10799.34it/s]\n",
      "84it [00:00, 6082.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for f in online_articles:\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l) + '</s>'\n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60fb4af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "buku_teks = [\n",
    "    'buku-teks.jsonl',\n",
    "    'bumigemilang.com.jsonl',\n",
    "    'mysoalan.com.jsonl',\n",
    "    'tcer.my.jsonl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2c440ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in buku_teks:\n",
    "    with open(f) as fopen:\n",
    "        for l in fopen:\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l) + '</s>'\n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59df54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_files = [\n",
    "    '/home/ubuntu/code_instructions_120k.jsonl.requested', \n",
    "    '/home/ubuntu/python_evol_instruct_51k.jsonl.requested',\n",
    "    '/home/ubuntu/unnatural-instructions.jsonl.requested',\n",
    "    '/home/ubuntu/glaive_coder_raw_text.jsonl.requested',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46fd6760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121623it [00:05, 20845.29it/s]\n",
      "51280it [00:06, 7855.12it/s] \n",
      "111504it [00:16, 6741.51it/s]\n",
      "136081it [00:15, 8782.77it/s] \n"
     ]
    }
   ],
   "source": [
    "for f in code_files:\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            l = json.loads(l)['r']\n",
    "            \n",
    "            if '```' not in l:\n",
    "                continue\n",
    "                \n",
    "            data = '<s>' + l + '</s>'\n",
    "            partitioned = partition(data)\n",
    "            for p in partitioned:\n",
    "                data = {\n",
    "                    'text': p,\n",
    "                }\n",
    "                a.write(f'{json.dumps(data)}\\n')\n",
    "                a.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7647be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "143000it [00:22, 6424.47it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('/home/ubuntu/WizardLM_evol_instruct_V2_143k.translated.jsonl') as fopen:\n",
    "    for l in tqdm(fopen):\n",
    "        l = json.loads(l)\n",
    "        conversations = l['conversations']\n",
    "        conversations = [c.get('value_ms') for c in conversations]\n",
    "        if any([c is None for c in conversations]):\n",
    "            continue\n",
    "        l = ' '.join(conversations)\n",
    "        if '```' not in l:\n",
    "            continue\n",
    "        data = '<s>' + l + '</s>'\n",
    "        partitioned = partition(data)\n",
    "        for p in partitioned:\n",
    "            data = {\n",
    "                'text': p,\n",
    "            }\n",
    "            a.write(f'{json.dumps(data)}\\n')\n",
    "            a.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ff18f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ubuntu/translated-gpt4all-code.json') as fopen:\n",
    "    code = json.load(fopen)\n",
    "    \n",
    "for c in code:\n",
    "    if len(c['prompt']) != 2:\n",
    "        continue\n",
    "    if len(c['response']) != 2:\n",
    "        continue\n",
    "    l = ' '.join([c['prompt'][1], c['response'][1]])\n",
    "    data = '<s>' + l + '</s>'\n",
    "    partitioned = partition(data)\n",
    "    for p in partitioned:\n",
    "        data = {\n",
    "            'text': p,\n",
    "        }\n",
    "        a.write(f'{json.dumps(data)}\\n')\n",
    "        a.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2e4481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_files = [\n",
    "    'math-qa.jsonl.translated',\n",
    "    'mini-math23k.jsonl.requested'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1984977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('math-qa.jsonl.translated') as fopen:\n",
    "    for l in fopen:\n",
    "        l = json.loads(l)\n",
    "        l = [l['Problem_ms'], l['options_ms'], l['Rationale_ms']]\n",
    "        if any([l_ is None for l_ in l]):\n",
    "            continue\n",
    "        l = ' '.join(l)\n",
    "        data = '<s>' + l + '</s>'\n",
    "        partitioned = partition(data)\n",
    "        for p in partitioned:\n",
    "            data = {\n",
    "                'text': p,\n",
    "            }\n",
    "            a.write(f'{json.dumps(data)}\\n')\n",
    "            a.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd3d6a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22918it [00:01, 12595.71it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('mini-math23k.jsonl.requested') as fopen:\n",
    "    for l in tqdm(fopen):\n",
    "        l = json.loads(l)\n",
    "        data = '<s>' + l['r']['result'] + '</s>'\n",
    "        partitioned = partition(data)\n",
    "        for p in partitioned:\n",
    "            data = {\n",
    "                'text': p,\n",
    "            }\n",
    "            a.write(f'{json.dumps(data)}\\n')\n",
    "            a.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b76d9a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "256504it [00:16, 15266.44it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('math-instruct.jsonl') as fopen:\n",
    "    for l in tqdm(fopen):\n",
    "        l = json.loads(l)\n",
    "        data = '<s>' + l['r']['result'] + '</s>'\n",
    "        partitioned = partition(data)\n",
    "        for p in partitioned:\n",
    "            data = {\n",
    "                'text': p,\n",
    "            }\n",
    "            a.write(f'{json.dumps(data)}\\n')\n",
    "            a.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2abba7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected = [\n",
    "    'markah untuk setiap satu',\n",
    "    'soalan mesti dijawab dalam',\n",
    "    '25 markah',\n",
    "    '50 markah'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ec1f305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "787it [00:09, 83.53it/s] \n",
      "189419it [03:29, 904.66it/s] \n"
     ]
    }
   ],
   "source": [
    "for f in research_papers:\n",
    "    f = os.path.join('/home/ubuntu/dedup-text-dataset', f)\n",
    "    with open(f) as fopen:\n",
    "        for l in tqdm(fopen):\n",
    "            try:\n",
    "                data = '<s>' + json.loads(l) + '</s>'\n",
    "                    \n",
    "                data_lower = data.lower()\n",
    "                if any([r in data_lower for r in rejected]):\n",
    "                    continue\n",
    "                    \n",
    "                partitioned = partition(data)\n",
    "                for p in partitioned:\n",
    "                    data = {\n",
    "                        'text': p,\n",
    "                    }\n",
    "                    a.write(f'{json.dumps(data)}\\n')\n",
    "                    a.flush()\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89d9b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c267f1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rwxrwxrwx 1 ubuntu ubuntu 31G Oct 18 12:42 combine-mistral.jsonl\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls -lh combine-mistral.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
